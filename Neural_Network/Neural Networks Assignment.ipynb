{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colab users uncomment this code\n",
    "#File for this notebook is \"NN_dataset.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "downloaded = drive.CreateFile({'id':'1R9vW5dmox7i8OGOoDql_r9yUrvAJgvhD'}) # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('NN_dataset.npz')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Assignment (70)\n",
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will be implementing a Neural Network from scratch to get a better understanding of how a Neural Network works and why does adding a hidden layer improve its performance.<br>\n",
    "In this assignment, you will be working with an artificially generated dataset. It is a small 2 dimensional $(\\therefore n_x = 2)$ dataset, involving a binary classification task. The two features of $X$ are the $x$ and $y$ co-ordinates on the 2D plane. $Y$ can have 2 values, $0$ and $1$ indicating the two classes.<br>\n",
    "The Assignment will be divided into 3 major sections:\n",
    "<ol>\n",
    "    <li><b>S1</b>: Applying a Logistic Regression model using sci-kit learn and see how it performs on the dataset.  (This step is already taken care for you)</li>\n",
    "    <li><b>S2</b>: Implementing a Neural Network with 1 hidden layer and compare its performance with the Logitic Regression.</li>\n",
    "    <li><b>S3</b>: Hyperparameter tuning to find the best fit for the dataset.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "<ul>\n",
    "    <li>S1 is non-evaluative.</li>\n",
    "    <li>The marks alloted to each step in S2 will be mentioned in the corresponding step. Just like numpy and pandas <b>60%</b> of marks will be allotted to <b>correctness of final answer</b> and <b>40%</b> of marks will be allotted to <b>conciseness of code</b>. As long as your code gives the correct output and doesn't have unnecessary code, you should be able to get full.</li>\n",
    "    <li>In S3 you will have to tune the parameters to better fit the dataset and try to get a high test accuracy. Then this model will be run on a hidden dataset during evaluation and the accuracy on the hidden dataset will be noted.</li>\n",
    "</ul>\n",
    "\n",
    "Finally the leaderboard will be ranked <b>primarily on total marks</b> and <b>secondarily on hidden set accuracy</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset.npz\n",
    "<b>Note</b>: You may have to change filepath if the file is not saved in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"NN_dataset.npz\")\n",
    "X_train = data[\"X_train\"]\n",
    "Y_train = data[\"Y_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "Y_test = data[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of Y_train: {Y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of Y_test: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[0,:], X_train[1, :], c = Y_train.ravel(), cmap = plt.cm.Wistia, linewidths = 0.8, edgecolors = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we will be needing a non-linear <b>Decision Boundary</b> to separate the 2 classes shown by 2 different colours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot the Decision Boundary\n",
    "\n",
    "<b>Decision Boundary</b> for a classification task refers to the hypersurface that divides the two classes. In a 2D plane for a binary classification task, it is a curve that divides the plane into two halves where the points in one half belong to one class while the points in the other half belong to the other class. The goal of a classification task is to learn this curve.<br>\n",
    "The function below plots the decision boundary for the fitted model. We'll use this to visualize the decision bundary learnt by our different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(predict_function, X, Y, trained_parameters = None):\n",
    "    xmin, xmax = X[0,:].min() - 0.25, X[0,:].max() + 0.25\n",
    "    ymin, ymax = X[1,:].min() - 0.25, X[1,:].max() + 0.25\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, 0.01), np.arange(ymin, ymax, 0.01))\n",
    "    if trained_parameters is None:\n",
    "        z = predict_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        z = predict_function(np.c_[xx.ravel(), yy.ravel()].T, trained_parameters)\n",
    "    z = z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, z, cmap=plt.cm.Wistia)\n",
    "    plt.scatter(X[0,:], X[1,:], c = Y.ravel(), cmap = plt.cm.Wistia, linewidths = 0.8, edgecolors = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use sci-kit learn to fit a Logistic Regression model on this dataset to see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Fitting the model\n",
    "clf = LogisticRegression(solver = 'lbfgs').fit(X_train.T, Y_train.ravel().T) #Specifying solver to suppress warning\n",
    "\n",
    "#Printing train and test set accuracy\n",
    "print(f\"Train set accruracy: {clf.score(X_train.T, Y_train.ravel().T)}\")\n",
    "print(f\"Test set accuracy: {clf.score(X_test.T, Y_test.ravel().T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, Logistic Regression performs badly on this data. Let's see why by visualizing the Decision Boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf.predict, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see Logistic Regression is limited to fitting a linear decision boundary and hence cannot fit this data very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Implementing an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "For this task, our Neural Network will have 1 Input Layer (with 2 units), 1 Ouput Layer (with 1 unit) and <b>1 Hidden Layer (with $n_H$ units)</b>. The Hidden Layer will have the <b>tanh</b> activation function while the Output Layer will have the <b>sigmoid</b> activation function. The cost function is <b>binary cross entropy</b> and we will use <b>Gradient Descent</b> as the optimization algorithm.<br>\n",
    "<center><img src = \"https://ik.imagekit.io/cpj5jrovil/Architecture_uLH5UKUC9.png\" width = \"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we'll define a set of helper functions that perform small tasks. We'll then put all these functions into another function to complete the model. Keep the 2 previous notebooks for your reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    <b>Note</b>:<br>\n",
    "    <ul>\n",
    "        <li>Make sure you write your code in <code>#Your code here</code> blocks.</li>\n",
    "        <li>Within functions, use only variables that are passed as parameters.</li>\n",
    "        <li>Do not modify function names or parameter names.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters (5)\n",
    "\n",
    "Define a function that initializes the parameters for a 2 Layer Neural network with <b>number of hidden units n_H</b>. The Weight matrices are initialized radomly to break symmetry. The Bias vectors are initialized with zeros.\n",
    "\n",
    "<b>Input</b>: X, Y and n_H.<br>\n",
    "<b>Ouput</b>: <b>parameters</b> (A list containing 4 variables in the following order [W1, b1, W2, b2]).<br>\n",
    "<b>Instructions</b>:\n",
    "<ul>\n",
    "    <li>Initialize Weight matrices with random numbers from a Standard Normal Distribution.</li>\n",
    "    <li>Initialize Bias vectors with zeros.</li>\n",
    "    <li>Make sure the shape of the matrices are correct for given value of n_H.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, Y, n_H):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    return [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function (5)\n",
    "\n",
    "Define a function that computes the elementwise sigmoid of a vector. For the tanh activation we can use NumPy's <code>np.tanh()</code> function.\n",
    "\n",
    "<b>Input</b>: Z (A vector)<br>\n",
    "<b>Output</b>: Elementwise sigmoid of Z (A vector with same dimensions as Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    #Your code here and replace None with return value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation (10)\n",
    "\n",
    "Define a function to perform a single step of forward propagation through the ANN.\n",
    "\n",
    "<b>Input</b>: X, parameters.<br>\n",
    "<b>Ouput</b>: <b>cache</b> (A list containing 4 variables in the following order [Z1, A1, Z2, A2]).<br>\n",
    "<b>Instructions</b>:\n",
    "<ul>\n",
    "    <li>Make sure you use the correct activation function for the correct layer.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, parameters):\n",
    "    \n",
    "    #Unpacking parameters\n",
    "    W1, b1, W2, b2 = parameters\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    return [Z1, A1, Z2, A2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost (5)\n",
    "\n",
    "Define a function that computes the binary cross entropy cost.\n",
    "\n",
    "<b>Input</b>: cache, Y.<br>\n",
    "<b>Ouput</b>: <b>J</b> (A single real number).<br>\n",
    "<b>Instructions</b>:\n",
    "<ul>\n",
    "    <li>Unpack the cache by following similar steps as shown above.</li>\n",
    "    <li>If you need $m$, think how you can obtain it from given inputs.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(cache, Y):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation (10)\n",
    "\n",
    "Define a function that performs a single step of backward propagation through the entire network.\n",
    "\n",
    "<b>Input</b>: X, Y, parameters and cache.<br>\n",
    "<b>Ouput</b>: <b>grads</b> (A list containing 4 variables in the following order [dW1, db1, dW2, db2]).<br>\n",
    "<b>Instructions</b>:\n",
    "<ul>\n",
    "    <li>Make sure you unpack parameters and cache.</li>\n",
    "    <li>For $\\sum\\limits_{col}$ make sure you set <code>keepdims = True</code> in <code>np.sum()</code> for NumPy broadcasting purposes.</li>\n",
    "    <li>In the first step of Backpropagation, instead of first computing $dA2$ and then computing $dZ2$, for sigmoid activation you can directly compute $dZ2 = A2 - Y$. Try figuring out why this works.</li>\n",
    "    <li>Verify that the shape of $\\theta$ and $d\\theta$ are same (where $\\theta$ is some variable).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(X, Y, parameters, cache):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    return [dW1, db1, dW2, db2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating parameters (5)\n",
    "\n",
    "Define a function that updates the parameters for a single step of Gradient Descent.\n",
    "\n",
    "<b>Input</b>: parameters, grads, learning_rate.<br>\n",
    "<b>Output</b>: parameters (Updated Values).<br>\n",
    "<b>Instructions</b>:\n",
    "<ul>\n",
    "    <li>Make sure you unpack parameters and grads.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    return [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model (15)\n",
    "\n",
    "It's finally time to put all your helper functions together and create the model. This function will take all the inputs required for the helper functions and it will output a list of 4 variables, the trained parameters.\n",
    "\n",
    "<b>Input</b>: X, Y, n_H, learning_rate, num_iterations.<br>\n",
    "<b>Ouput</b>: parameters (Trained parameters).<br>\n",
    "<b>Instructions</b>:\n",
    "<ul>\n",
    "    <li>Make sure you properly use the helper functions defined above.</li>\n",
    "    <li><b>Costs</b>:\n",
    "        <ul>\n",
    "            <li>Create an empty list in the beginning and call it <code>costs</code>. After every iteration of Gradient Descent append the cost computed in that iteration to this empty list.</li>\n",
    "            <li>Print the cost after every 100 iterations in a presentable format.</li>\n",
    "            <li>Using <code>costs</code> plot the cost v/s iterations graph.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_H, learning_rate, num_iterations):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've finished writing this function, execute the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_parameters = model(X_train, Y_train, 3, 1, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works fine, the graph should be continuously decreasing. If this is not the case, there might be some error in some helper function or your model. Try fixing that. If everything worked fine move ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict (10)\n",
    "\n",
    "Define a function that predicts the class given an input X. If the Neural Network ouputs a value greater than or equal to 0.5, the class is 1 and if it outputs a value less than 0.5, the class is 0.\n",
    "\n",
    "<b>Input</b>: X, trained_prameters.<br>\n",
    "<b>Output</b>: Predictions.<br>\n",
    "<b>Hint</b>: You will use 1 helper function in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, trained_parameters):\n",
    "    \n",
    "    #Your code here and replace None with return value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (5)\n",
    "\n",
    "Define a function that computes accuracy.\n",
    "\n",
    "<b>Input</b>: Predictions, True Value.<br>\n",
    "<b>Ouput</b>: Accuracy (a single real number).<br>\n",
    "<b>Hint</b>: Remember that, $$\\text{Accuracy} = \\frac{\\text{Number of values predicted correctly}}{\\text{Total number of values}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, actual):\n",
    "    \n",
    "    #Your code here and replace None with return value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this piece of code to find the training and test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set accruracy: {accuracy(predict(X_train, trained_parameters), Y_train)}\")\n",
    "print(f\"Test set accuracy: {accuracy(predict(X_test, trained_parameters), Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the accuracy of ANN is clearly far better than the accuracy of Logistic Regression. Let's see the Decision Boundary to see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(predict, X_test, Y_test, trained_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the addition of a single hidden layer gives the ANN the capability of fitting a non-linear decision boundary. The activation function in the hidden layer adds this Non-Linear factor to the model. By adding more hidden layers we can fit even more complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 5.3 of the last notebook, we briefly saw what hyperparameters are. Hyperparameters are features of the model that you manually change to better fit the model to given data. The hyperparameters thay we can vary in this model are:\n",
    "<ul>\n",
    "    <li>Number of hidden units (n_H)</li>\n",
    "    <li>Learning Rate (learning_rate)</li>\n",
    "    <li>Number of iterations (num_iterations)</li>\n",
    "</ul>\n",
    "So go ahead and change the values of the hyperparameters to see how the accuracy and decision boundary change as you vary the hyperparameters. Also think about why these values change the way they do. Once you get a hang of it, try to get a high test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    <b>Note</b>:<br>\n",
    "    <ul>\n",
    "        <li>It is the <b>test</b> set accuracy that actually matters. So make sure your test set accuracy is close to the training accuracy and is not very less compared to the latter (this case is called <b>overfitting</b>). It is okay if test set accuracy is higher than the training accuracy.</li>\n",
    "        <li>Your cost v/s iterations curve should be a smooth decreasing curve.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_parameters = model(X_train, Y_train, 3, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set accruracy: {accuracy(predict(X_train, trained_parameters), Y_train)}\")\n",
    "print(f\"Test set accuracy: {accuracy(predict(X_test, trained_parameters), Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(predict, X_test, Y_test, trained_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final model of yours will be run on a hidden dataset and the accuracy will be noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border-width:2;color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Job on completing this Assignment!!<br>\n",
    "As you would have experienced, implementing a Neural Network from scratch is very painful. As you build deeper Neural Networks, you need to compute gradients for each of those steps and code it. In fact there are Neural Networks that go as deep as 150 layers! Imagine coding all that from scratch.<br>\n",
    "That's where Machine Learning Frameworks come in to make our lives easier. All you need to do is set up the Neural Network architecture by defining the number of layers, number of hidden units etc. and you are ready to train it. The framework will take care of the hectic job of Backpropagation.<br>\n",
    "In the next topic you'll be introduced to one such framework, Keras. So, Happy Learning !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:2;color:gray\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
