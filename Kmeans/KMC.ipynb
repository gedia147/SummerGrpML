{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "KMC.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5BwW9sqK5kU"
      },
      "source": [
        "# K MEANS CLUSTERING\n",
        "<hr style=\"height:5px;border-width:2;color:gray\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYab0-0LK5ki"
      },
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "So far we have seen different models like Linear Regression, Logistic Regression and Neural Networks. However if you remember from the Introduction to ML document all these are Supervised Learning models, where we have both input and output and our goal is to create a logic associating input and the output. However sometimes you may deal only with inputs which may have no defined outputs. This is called Unsupervised Learning.\n",
        "\n",
        "For example,\n",
        "<ul>\n",
        "    <li>Dimensionality Reduction (Ex: PCA, tSNE)</li>\n",
        "    <li>Clustering (Ex: K Means, Hierarchical Clustering, DBSCAN)</li>\n",
        "</ul>\n",
        "etc.\n",
        "<br>\n",
        "Here we'll be looking at the most basic clustering algorithm, K Means Clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0lidfxlK5kk"
      },
      "source": [
        "<hr style=\"height:2px;border-width:2;color:gray\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxet57SvK5km"
      },
      "source": [
        "## Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayKd6vBkK5ko"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "K Means is one of the simplest well known unsupervised clustering algorithms. A <b>cluster</b> is a group of data points that are similar to each other in some way. Generally to find similarity, Euclidean distance between the features is used i.e, the data points which are closer to each other are more similar. \n",
        "\n",
        "In K means, <b>K</b> refers to <b>number of clusters</b>.\n",
        "\n",
        "The main goal of K means is to find <b>K</b> points and call these the center of the clusters. After that each data point is assigned to cluster whose center is closest to the data point. The K Means Algorithm is used to find these centers.\n",
        "\n",
        "<center><img src = \"https://i2.wp.com/cmdlinetips.com/wp-content/uploads/2019/05/kmeans_data.png?resize=432%2C288\"></center>\n",
        "\n",
        "> In this image, the data points have two features and are plotted on the X and Y axis. The different clusters are denoted by different colours. As we can see, the points belonging to same cluster are closer to each other.<br><a href = \"https://cmdlinetips.com/2019/05/k-means-clustering-in-python/\">Image credits</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SEcsPRtK5kq"
      },
      "source": [
        "### Algorithm\n",
        "\n",
        "Let $\\{ v_1, v_2 \\ldots v_m\\}$ be $m$ data points that we need to cluster.\n",
        "\n",
        "<b><u>Algorithm Steps</u></b>:<br>\n",
        "<ol>\n",
        "    <li>\n",
        "        <b>Chose a suitable K value i.e, number of clusters.</b> \n",
        "    </li>\n",
        "    <li>\n",
        "        <b>Randomly chose K cluster centers</b>:<br>\n",
        "        Randomly chose K data points and call them the cluster centers $\\{C_1, C_2 \\ldots C_K\\}$.\n",
        "    </li>\n",
        "    <li>\n",
        "        <b>Assign cluster to each data point</b>:<br>\n",
        "        For each data point $v_i$ find the closest cluster center to it and assign $v_i$ to that cluster. The closeness between two points is measured by the Euclidean distance between the two points.\n",
        "    </li>\n",
        "    <li>\n",
        "        <b>Recalculate the cluster centers</b>:\n",
        "        Each data point has been assigned to a single cluster in the previous step. Now for each <u>cluster</u> (say $C_j$), calculate the mean of all the data points that belong to $C_j$. Reassign $C_j$ to this mean i.e,<br><br>\n",
        "        $$C_j = mean(\\{v_i \\ | \\  v_i \\in C_j\\})$$\n",
        "    <br>\n",
        "    </li>\n",
        "    <li>\n",
        "        Repeat the process from step 3 until maximum number of iterations is reached or cluster centers stop changing.\n",
        "    </li>\n",
        "</ol>\n",
        "\n",
        "Since the final clusters depend on initially chosen cluster centers (which are chosen at random) the entire algorithm is applied multiple times to find the best results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HM0lu4zK5ks"
      },
      "source": [
        "### Intuition\n",
        "\n",
        "In the first step, we randomly chose K cluster centers. They are generally chosen as K random data points instead of random points in the entire space. After that, the algorithm moves the cluster centers closer and closer to the actual cluster centers in every iteration, until the cluster centers don't change anymore. However the final clusters will depend on initial random initialization and hence we run the algorithm multiple times to chose the best clusters.\n",
        "\n",
        "The following image shows K Means at various steps.\n",
        "\n",
        "<center><img src = \"https://stanford.edu/~cpiech/cs221/img/kmeansViz.png\"></center>\n",
        "\n",
        "> <a href = \"https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\">Image credits</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBzKvxe7K5kt"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Since K Means is an unsupervised learning algorithm, we can't measure how well the algorithm performs by comparing it to a true value. So, the clusters are said to be good if the <b>data points belonging to same cluster are close to each other</b> and <b>data points belonging to different clusters are far from each other</b>.\n",
        "\n",
        "Therefore, a K Means model is evaluated using the sum of squared distances between every data point and its cluster center. The smaller the SSE (Sum of Squared Errors), the better the clustering. To find SSE, for each cluster find the sum of the squared distances between the data points that belong to that cluster and the cluster centers. Then add all these sums for the K clusters.\n",
        "\n",
        "To chose the best clustering, run the K Means algorithm multiple times and chose the cluster center initialization that gives the lowest SSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMsmvWIjK5kv"
      },
      "source": [
        "### Chosing the best K Value\n",
        "\n",
        "We can also use the SSE to find the best K value using a method called the <b>elbow method</b>. \n",
        "\n",
        "To do this, plot the SSE (by SSE we mean SSE for the best clustering) against K values. The SSE drops as K value increases (in fact, SSE is 0 when K is equal to number of data points). So the K value for which there is a maximum drop in SSE is the best estimate for number of clusters. After this maximum drop, the curve seems to slowly flatten.\n",
        "\n",
        "<div class = \"alert alert-block alert-info\">\n",
        "    <b>Note</b>: Sometimes this method may not work as it might be difficult to identify the elbow point.\n",
        "</div>\n",
        "\n",
        "<center><img src = \"https://miro.medium.com/max/866/1*9z8erk4kvsnxkfv-QhsHZg.png\"></center>\n",
        "\n",
        "> In this image K = 2 causes maximum drop in SSE. Also notice how the curve tends to slowly flatten after K = 2. This methid is called elbow method because the curve looks like a human arm and the optimal value is the elbow point.<br>\n",
        "<a href = \"https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\">Image credits</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De3tafmfK5kw"
      },
      "source": [
        "### Advantages and Disadvantages\n",
        "\n",
        "#### Advantages\n",
        "<ul>\n",
        "    <li>It is fast and simple to understand.</li>\n",
        "    <li>Works really well when clusters are distinct.</li>\n",
        "</ul>\n",
        "\n",
        "#### Disadvantages\n",
        "<ul>\n",
        "    <li>Need to manually chose K value.</li>\n",
        "    <li>The algorithm may sometimes end up in local optima.</li>\n",
        "    <li>The algorithm does not works very well only for circular clusters.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDKXVbjKK5kz"
      },
      "source": [
        "<hr style=\"height:2px;border-width:2;color:gray\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urg653X6K5k0"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "We can implement K Means clustering using Scikit Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKBuPQm9K5k1"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICPvT3wgK5k3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8lSno6K5k6"
      },
      "source": [
        "### Creating the dataset\n",
        "\n",
        "An artificial cluster dataset can be created using sklearn. Also one more thing to notice is that generally we don't do train test splits in clustering since we don't have any labels to test on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjBx-R4pK5k8"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "data,_ = make_blobs(200, n_features = 2, centers = 3, cluster_std = [1, 1.5, 2], random_state = 1234)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqkntpKMK5k8"
      },
      "source": [
        "Let's plot the given data. First feature on X-axis and second feature on Y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjuDcZI5K5k9"
      },
      "source": [
        "sns.scatterplot(data[:,0], data[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMxW4-qlK5k-"
      },
      "source": [
        "From the figure we can make out 3 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ridb-EdXK5k-"
      },
      "source": [
        "### Creating the model\n",
        "\n",
        "Create a K Means clustering model using sklearn. After fitting the model, plot the data points and distinguish the clusters by color. Also plot the cluster centers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX0gab8HK5k_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y81cSEFK5lA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymjNSZSTK5lA"
      },
      "source": [
        "#### Experimentation\n",
        "\n",
        "The KMeans class abstracts a lot of information. Try looking at all the attributes of the class and try changing them. For example, <code>init</code> is set to \"k-means++\" initially which intelligently choses initial clusters, try changing it to \"random\"; <code>n_init</code> is set to 10 by default which means the algorithm is run 10 times and the best one is chosen automatically, try changing it to 1 and run the model multiple times.\n",
        "\n",
        "Also here we know that K = 3 because this data is artificially generated. Try fitting the model for different K values and see how KMeans tries to fit for those K values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwDwl1wK5lB"
      },
      "source": [
        "### Elbow plot\n",
        "\n",
        "Since this data is 2 dimensional we could easily figure out that there were 3 clusters. However in case of high dimensional data we can use an elbow plot to figure out the number of clusters.\n",
        "\n",
        "Plot an elbow plot for this data. (<b>Hint</b>: You can use sklearn to calculate SSE for each K value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbFJH4JQK5lC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s05bTs0DK5lC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7vZw5R9K5lD"
      },
      "source": [
        "If you see the elbow plot, you can notice that the elbow point is at K = 3. Eventhough K = 2 has a very high drop, the curve starts to flatten after K = 3 and hence it is the elbow point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPkLMBbbK5lD"
      },
      "source": [
        "#### Experimentation\n",
        "\n",
        "Feel free to experiment by changing the number of clusters in <code>make_blobs</code> and see how KMeans performs on different data. Also try to see if Standardization has any effect on KMeans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8O-wAP_K5lE"
      },
      "source": [
        "<hr style=\"height:2px;border-width:2;color:gray\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_N97vNsK5lF"
      },
      "source": [
        "### Resources for further learning\n",
        "\n",
        "<ul>\n",
        "    <li>Checkout this <a href = \"https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\">article</a> for more information on K Means.</li>\n",
        "    <li><a href = \"https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN\">This</a> is the link to Andrew Ng's Machine Learning playlist on youtube. He discusses Unsupervised Learning from <b>Lecture 13.1</b>. He not only covers K Means and clustering but also covers other Unsupervised Learning techniques like Dimensionality reduction, Anomaly detection, etc.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iW6IFABK5lH"
      },
      "source": [
        "<hr style=\"height:5px;border-width:2;color:gray\">"
      ]
    }
  ]
}